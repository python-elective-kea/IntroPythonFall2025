{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Øvelse: Text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du har i forberedelsen til i dag set videoen OpenAI Embeddings and Vector Databases Crash Course. Hvis du ikke selv har prøvet det han viser i videoen skal du gøre det nu.\n",
    "Herefter brug Mistral.ai embeddings (mistral-embed) til at lave det samme som i videoen, men nu skal du kontakte API´et gennem python kode du selv laver.\n",
    "I stedet for den database han bruger i videoen skal du bruge ChromaDB. Det er en lokal database og det du har brug for at arbejde med kan du læse om her:\n",
    "\n",
    "* Getting Started\n",
    "* Persistent Client\n",
    "\n",
    "Du vil lægge mærke til at ChromaDB som default kan embeded dokumenter \"af sig selv\". Dens Default embeddings model er Sentence Transformers all-MiniLM-L6-v2. Det betyder at du faktisk ikke behøver at bruge Mistral eller ChatGpt´s API´er. Prøv at få dit program til at virke med dette deafault set up.\n",
    "\n",
    "Læg mærke til at din ChromaDB er gemt som en sqlite3 fil. Prøv at installer Extension SQLite Viewerog undersøg dit ChromaDB i gennem det."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dette svare nogenlunde til hvad der bliver gennemgået i [videoen](https://www.youtube.com/watch?v=ySus5ZS0b94), men med brug af chromadb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(name=\"exerciseembeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=[\n",
    "        \"Hello world\",\n",
    "        \"MistralAI Vectors and Embeddings are easy!\",\n",
    "        \"\"\"This latest generation continues to push the boundaries of cost efficiency, speed, and performance. Mistral Large 2 is exposed on la Plateforme and enriched with new features to facilitate building innovative AI applications.\n",
    "\n",
    "Mistral Large 2\n",
    "Mistral Large 2 has a 128k context window and supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash.\n",
    "\n",
    "Mistral Large 2 is designed for single-node inference with long-context applications in mind – its size of 123 billion parameters allows it to run at large throughput on a single node. We are releasing Mistral Large 2 under the Mistral Research License, that allows usage and modification for research and non-commercial usages. For commercial usage of Mistral Large 2 requiring self-deployment, a Mistral Commercial License must be acquired by contacting us.\n",
    "\n",
    "General performance\n",
    "Mistral Large 2 sets a new frontier in terms of performance / cost of serving on evaluation metrics. In particular, on MMLU, the pretrained version achieves an accuracy of 84.0%, and sets a new point on the performance/cost Pareto front of open models.\n",
    "\n",
    "Code & Reasoning\n",
    "Following our experience with Codestral 22B and Codestral Mamba, we trained Mistral Large 2 on a very large proportion of code. Mistral Large 2 vastly outperforms the previous Mistral Large, and performs on par with leading models such as GPT-4o, Claude 3 Opus, and Llama 3 405B.\n",
    "\n",
    "\"\"\"\n",
    "    ],\n",
    "    ids=[\"id1\", \"id2\", \"id3\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id1', 'id3']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Hello world',\n",
       "   'This latest generation continues to push the boundaries of cost efficiency, speed, and performance. Mistral Large 2 is exposed on la Plateforme and enriched with new features to facilitate building innovative AI applications.\\n\\nMistral Large 2\\nMistral Large 2 has a 128k context window and supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash.\\n\\nMistral Large 2 is designed for single-node inference with long-context applications in mind – its size of 123 billion parameters allows it to run at large throughput on a single node. We are releasing Mistral Large 2 under the Mistral Research License, that allows usage and modification for research and non-commercial usages. For commercial usage of Mistral Large 2 requiring self-deployment, a Mistral Commercial License must be acquired by contacting us.\\n\\nGeneral performance\\nMistral Large 2 sets a new frontier in terms of performance / cost of serving on evaluation metrics. In particular, on MMLU, the pretrained version achieves an accuracy of 84.0%, and sets a new point on the performance/cost Pareto front of open models.\\n\\nCode & Reasoning\\nFollowing our experience with Codestral 22B and Codestral Mamba, we trained Mistral Large 2 on a very large proportion of code. Mistral Large 2 vastly outperforms the previous Mistral Large, and performs on par with leading models such as GPT-4o, Claude 3 Opus, and Llama 3 405B.\\n\\n']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[None, None]],\n",
       " 'distances': [[1.3331246157369148, 1.4629015118134012]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"OpenAI\"], # Chroma will embed this for you\n",
    "    n_results=2 # how many results to return\n",
    ")\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
